{"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"tf2_py39"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text\/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.9.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"693f34aa4da25eb5054a838b13b5bf778a50ee7285c932f2b87b210dcb0b1574"}}},"cells":[{"metadata":{"collapsed":false},"attachments":{},"source":["# KQM example\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":1,"source":["'''\n","Create a 2 moons dataset \n","'''\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split\n","\n","# Create a 2 moons dataset\n","X, y = make_moons(n_samples=1000, noise=0.2, random_state=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":2,"source":["# Function to visualize a 2D dataset\n","def plot_data(X, y):\n","    y_unique = np.unique(y)\n","    colors = plt.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\n","    for this_y, color in zip(y_unique, colors):\n","        this_X = X[y == this_y]\n","        plt.scatter(this_X[:, 0], this_X[:, 1],  c=color,\n","                    alpha=0.5, edgecolor='k',\n","                    label=\"Class %s\" % this_y)\n","    plt.legend(loc=\"best\")\n","    plt.title(\"Data\")\n","\n","# Function to visualize the decission surface of a classifier\n","def plot_decision_region(X, pred_fun):\n","    min_x = np.min(X[:, 0])\n","    max_x = np.max(X[:, 0])\n","    min_y = np.min(X[:, 1])\n","    max_y = np.max(X[:, 1])\n","    min_x = min_x - (max_x - min_x) * 0.05\n","    max_x = max_x + (max_x - min_x) * 0.05\n","    min_y = min_y - (max_y - min_y) * 0.05\n","    max_y = max_y + (max_y - min_y) * 0.05\n","    x_vals = np.linspace(min_x, max_x, 50)\n","    y_vals = np.linspace(min_y, max_y, 50)\n","    XX, YY = np.meshgrid(x_vals, y_vals)\n","    grid_r, grid_c = XX.shape\n","    vals = [[XX[i, j], YY[i, j]] for i in range(grid_r) for j in range(grid_c)]\n","    preds = pred_fun(np.array(vals))\n","    ZZ = np.reshape(preds, (grid_r, grid_c))\n","    print(np.min(preds), np.min(ZZ))\n","    plt.contourf(XX, YY, ZZ, 100, cmap = plt.cm.coolwarm, vmin= 0, vmax=1)\n","    plt.colorbar()\n","    CS = plt.contour(XX, YY, ZZ, 100, levels = [0.1*i for i in range(1,10)])\n","    plt.clabel(CS, inline=1, fontsize=10)\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"y\")\n","\n","def gen_pred_fun(clf):\n","    def pred_fun(X):\n","        return clf.predict(X)[:, 1]\n","    return pred_fun\n","\n","plot_data(X_train, y_train)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["## A shallow model \n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":3,"source":["from keras.layers import Input, Dense\n","from keras.models import Model\n","from keras import optimizers\n","from keras import losses\n","from keras import metrics\n","import kqm\n","import tensorflow as tf\n","\n","from importlib import reload\n","reload(kqm);\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["A shallow model feeds directly the input to a KQMUnit with an 2D input and a 2D output. It uses a RBF kernel that compute the similarity between the input and the prototypes of the KQMUnit. The prototypes are initialized with random samples from the training set. The output of the KQMUnit is a discrete probability distribution represented by $\\rho_y$.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":4,"source":["# Create a and encoder model that maps from 2D to 2D\n","\n","# This returns a tensor\n","inputs = Input(shape=(2,))\n","\n","# Create a classifier model using a KQMUnit\n","n_comp = 16 # number of components or prototypes\n","sigma = 0.1 # initial value of the kernel sigma parameter\n","\n","# components\n","kernel = kqm.RBFKernelLayer(sigma=sigma, dim=2) \n","kqm_unit = kqm.KQMUnit(kernel=kernel, dim_x=2, dim_y=2, n_comp=n_comp) \n","\n","# model\n","rho_x = kqm.pure2dm(inputs) # convert the input to a density matrix representing a pure state\n","rho_y = kqm_unit(rho_x) # apply the KQM unit\n","probs = kqm.dm2discrete(rho_y) # convert the output density matrix to a discrete probability distribution\n","kqm_class = Model(inputs, probs)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":5,"source":["\n","# train the classifier\n","kqm_class.compile(optimizer=optimizers.Adam(lr=1e-3),\n","                    loss=losses.sparse_categorical_crossentropy,\n","                    metrics=[metrics.sparse_categorical_accuracy])\n","idx = np.random.randint(X_train.shape[0], size=n_comp)\n","kqm_unit.c_x.assign(X_train[idx]) # initialize the components with random samples from the training set\n","kqm_class.fit(X_train, y_train, epochs=40, batch_size=32, verbose=0)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":6,"source":["plot_decision_region(X, gen_pred_fun(kqm_class))\n","plot_data(X_train, y_train)\n","# plot the prototypes\n","plt.scatter(kqm_unit.c_x.numpy()[:, 0], kqm_unit.c_x.numpy()[:, 1], c='y', marker='X')\n","# Evaluate the classifier\n","score = kqm_class.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","score = kqm_class.evaluate(X_train, y_train, verbose=0)\n","print('Train loss:', score[0])\n","print('Train accuracy:', score[1])\n","print(f'Sigma: {kernel.sigma.numpy()}')\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":7,"source":["kqm_unit.comp_w\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["## A dense model\n","\n","Now we add an encoder to find a representation of the input. The encoder is a simple MLP with 2 hidden layers. The output of the encoder is a 2D vector that is fed to the KQMUnit. The output of the KQMUnit is a discrete probability distribution represented by $\\rho_y$. We use only 4 prototypes for the KQMUnit to force the encoder to learn a discriminative representation.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":8,"source":["# Create an encoder model that maps from 2D to 2D\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","\n","# This returns a tensor\n","inputs = Input(shape=(2,))\n","# a layer instance is callable on a tensor, and returns a tensor\n","encoded = Dense(8, activation='relu')(inputs)\n","encoded = Dense(2, activation='tanh')(encoded)\n","\n","# This creates a model that includes\n","# the Input layer and three Dense layers\n","encoder = Model(inputs, encoded)\n","\n","# Create a classifier model using a KQMUnit\n","import kqm\n","import tensorflow as tf\n","n_comp = 2 # number of components or prototypes\n","sigma = tf.Variable(0.1, dtype=tf.float32)\n","#sigma = 0.1\n","kernel = kqm.RBFKernelLayer(sigma=sigma, dim=2)\n","kqm_unit = kqm.KQMUnit(kernel=kernel, dim_x=2, dim_y=2, n_comp=n_comp)\n","rho_x = kqm.pure2dm(encoded)\n","rho_y = kqm_unit(rho_x)\n","probs = kqm.dm2discrete(rho_y)\n","kqm_class = Model(inputs, probs)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":9,"source":["encoder.trainable = True\n","kqm_class.compile(optimizer=optimizers.Adam(lr=5e-4),\n","                    loss=losses.sparse_categorical_crossentropy,\n","                    metrics=[metrics.sparse_categorical_accuracy])\n","kqm_class.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":10,"source":["plot_decision_region(X, gen_pred_fun(kqm_class))\n","plot_data(X_train, y_train)\n","# Evaluate the classifier\n","score = kqm_class.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","score = kqm_class.evaluate(X_train, y_train, verbose=0)\n","print('Train loss:', score[0])\n","print('Train accuracy:', score[1])\n","print(f'Sigma: {kernel.sigma.numpy()}')\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":11,"source":["'''\n","Plot the points in the feature space induced by the encoder\n","'''\n","# Visualize the points in the feature space\n","plt.scatter(encoder(X_train)[:, 0], encoder(X_train)[:, 1], alpha=0.5, c=y_train, cmap=plt.cm.coolwarm)\n","# plot the prototypes\n","plt.scatter(kqm_unit.c_x.numpy()[:, 0], kqm_unit.c_x.numpy()[:, 1], c='k', marker='X', s=100)\n","plt.show()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":12,"source":["kqm_unit.comp_w.numpy()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["## MLE training \n","\n","We use another alternative to train the model, maximum likelihood estimation (MLE) of the joint probabilities of the input and the output. For this we use a KQMOverlap unit that computes the overlap between the input and the prototypes (kernel). The output of the KQMOverlap is a continuous value that represents an unnormalized density that is normalized using the corresponding normalization factors of the input and output kernels. This model is codified in the KQMDenEstModel2 class.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":13,"source":["# join X and y using a one-hot encoding for y\n","Xy_train = np.concatenate((X_train, np.eye(2)[y_train]), axis=1)\n","Xy_test = np.concatenate((X_test, np.eye(2)[y_test]), axis=1)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":14,"source":["n_comp = 16 # number of components or prototypes\n","kqmd_model2 = kqm.KQMDenEstModel2(2, 2, 0.1, n_comp=n_comp) # Density estimation model for joint input and output\n","optimizer = tf.keras.optimizers.Adam(lr=5e-3)\n","kqmd_model2.compile(optimizer=optimizer)\n","kqmd_model2.predict(Xy_train[:1]) # initialize the model\n","idx = np.random.randint(Xy_train.shape[0], size=n_comp)\n","kqmd_model2.kqmover.c_x.assign(Xy_train[idx]) # initialize the components with random samples from the training set\n","\n","# Plot initial prototypes\n","centroids = kqmd_model2.kqmover.c_x.numpy()\n","fig = plt.figure(figsize=(4, 3))\n","plt.scatter(Xy_train[:, 0], Xy_train[:, 1], alpha=0.5)\n","plt.scatter(centroids[:, 0], centroids[:, 1], marker='o')\n","plt.show()\n","kqmd_model2.fit(Xy_train, epochs=100, verbose=0, batch_size=100)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":15,"source":["\n","# Plot final prototypes\n","fig = plt.figure(figsize=(4, 3))\n","plt.scatter(Xy_train[:, 0], Xy_train[:, 1], alpha=0.5)\n","\n","#comp_w = tf.clip_by_value(kqmd_model2.kqmover.comp_w, 1e-10, 1).numpy()\n","comp_w = np.abs(kqmd_model2.kqmover.comp_w.numpy())\n","comp_w = comp_w \/ tf.reduce_sum(comp_w)\n","prototypes = kqmd_model2.kqmover.c_x.numpy()[comp_w > 0.01] # only plot the prototypes with a weight above 0.01\n","plt.scatter(prototypes[:, 0], prototypes[:, 1], \n","             marker='o')\n","#plt.scatter(centroids[:, 0], centroids[:, 1], marker='o')\n","plt.show()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":16,"source":["kqmd_model2.kqmover.comp_w.numpy()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["We create a classifier model based on the density estimation model. The classifier model uses a KQMUnit with parameters initialized with the parameters of the KQMOverlap unit. \n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":17,"source":["# This returns a tensor\n","inputs = Input(shape=(2,))\n","\n","# Create a classifier model using a KQMUnit\n","sigma = kqmd_model2.kernel_x.sigma\n","\n","# components\n","kernel = kqm.RBFKernelLayer(sigma=sigma, dim=2)\n","kqm_unit = kqm.KQMUnit(kernel=kernel, dim_x=2, dim_y=2, n_comp=n_comp)\n","\n","# model\n","rho_x = kqm.pure2dm(inputs)\n","rho_y = kqm_unit(rho_x)\n","probs = kqm.dm2discrete(rho_y)\n","kqm_class = Model(inputs, probs)\n","\n","\n","# Use the parameters of kqmd_model2 to initialize the parameters of the classifier model\n","kqm_unit.c_x.assign(kqmd_model2.kqmover.c_x[:, 0:2]) \n","kqm_unit.c_y.assign(kqmd_model2.kqmover.c_x[:, 2:4])\n","kqm_unit.comp_w.assign(kqmd_model2.kqmover.comp_w)\n","kernel.sigma.assign(kqmd_model2.kernel_x.sigma)\n","\n","kqm_class.compile(optimizer=optimizers.Adam(lr=1e-3),\n","                    loss=losses.sparse_categorical_crossentropy,\n","                    metrics=[metrics.sparse_categorical_accuracy])\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":18,"source":["plot_decision_region(X, gen_pred_fun(kqm_class))\n","plot_data(X_train, y_train)\n","# plot the prototypes\n","comp_w = tf.clip_by_value(kqm_unit.comp_w, 1e-10, 1).numpy()\n","comp_w = comp_w \/ tf.reduce_sum(comp_w)\n","prototypes = kqm_unit.c_x.numpy()[comp_w > 0.01]\n","plt.scatter(prototypes[:, 0], prototypes[:, 1], \n","             c='y', marker='X')\n","# Evaluate the classifier\n","score = kqm_class.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","score = kqm_class.evaluate(X_train, y_train, verbose=0)\n","print('Train loss:', score[0])\n","print('Train accuracy:', score[1])\n","print(f'Sigma: {kernel.sigma.numpy()}')\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["The above visualization shows the decision boundary of the classifier model along with the prototypes of the KQMUnit. Only prototypes with a weight greater than 0.01 are shown. \n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"attachments":{},"source":["## MNIST example\n","\n","An example using the mnist dataset. In addition to perform classification we will show how to perform generation.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":19,"source":["from keras.layers import Input, Dense\n","from keras.models import Model\n","from keras import optimizers\n","from keras import losses\n","from keras import metrics\n","import kqm\n","import tensorflow as tf\n","\n","from importlib import reload\n","reload(kqm);\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":20,"source":["# Create a dataset from mnist using tf\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","X_train = X_train.astype('float32') \/ 255.\n","X_test = X_test.astype('float32') \/ 255.\n","\n","\n","# reshape the data to include a channel dimension\n","X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n","X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n","\n","# Partition the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n","\n","def display_imgs(x, y=None):\n","    if not isinstance(x, (np.ndarray, np.generic)):\n","      x = np.array(x)\n","    plt.ioff()\n","    n = x.shape[0]\n","    fig, axs = plt.subplots(1, n, figsize=(n, 1))\n","    if y is not None:\n","      fig.suptitle(np.argmax(y, axis=1))\n","    for i in range(n):\n","      axs.flat[i].imshow(x[i].squeeze(), interpolation='none', cmap='gray')\n","      axs.flat[i].axis('off')\n","    plt.show()\n","    plt.close()\n","    plt.ion()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"source":["A deep encoder:\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":21,"source":["tfkl = tf.keras.layers\n","\n","def create_encoder(input_shape, base_depth, encoded_size):\n","    encoder = tf.keras.Sequential([\n","        tfkl.InputLayer(input_shape=input_shape),\n","        tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n","        tfkl.Conv2D(base_depth, 5, strides=1,\n","                    padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2D(base_depth, 5, strides=2,\n","                    padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2D(2 * base_depth, 5, strides=1,\n","                    padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2D(2 * base_depth, 5, strides=2,\n","                    padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2D(4 * encoded_size, 7, strides=1,\n","                    padding='valid', activation=tf.nn.leaky_relu),\n","        tfkl.Dense(encoded_size,\n","                activation=None),#, activity_regularizer=tf.keras.regularizers.l2(1e-3)),\n","        #tfk.layers.LayerNormalization(),\n","        tfkl.Flatten(),\n","    ])\n","    return encoder\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":22,"source":["# Create a  encoder model that maps from 2D to 2D\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","\n","input_shape = (28, 28, 1)\n","base_depth = 32\n","encoded_size = 2\n","\n","# a layer instance is callable on a tensor, and returns a tensor\n","encoder = create_encoder(input_shape, base_depth, encoded_size)\n","\n","# Create a classifier model using a KQMUnit\n","n_comp = 64\n","\n","inputs = Input(shape=input_shape)\n","encoded = encoder(inputs)\n","sigma = 0.1\n","kernel = kqm.RBFKernelLayer(sigma=sigma, dim=2)\n","kqm_unit = kqm.KQMUnit(kernel=kernel, dim_x=encoded_size, dim_y=10, n_comp=n_comp)\n","rho_x = kqm.pure2dm(encoded)\n","rho_y = kqm_unit(rho_x)\n","probs = kqm.dm2discrete(rho_y)\n","kqm_class = Model(inputs, probs)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":23,"source":["from sklearn.metrics import pairwise_distances\n","\n","encoder.trainable = True\n","kqm_class.compile(optimizer=optimizers.Adam(lr=5e-4),\n","                    loss=losses.sparse_categorical_crossentropy,\n","                    metrics=[metrics.sparse_categorical_accuracy])\n","\n","\n","# initialize the prototypes using random samples from the training set\n","idx = np.random.randint(X_train.shape[0], size=n_comp)\n","kqm_unit.c_x.assign(encoder(X_train[idx]))\n","kqm_unit.c_y.assign(tf.one_hot(y_train[idx], 10))\n","\n","# initialize the kernel sigma parameter using the mean pairwise distance of prototypes\n","distances = pairwise_distances(encoder(X_train[idx]).numpy())\n","sigma = np.mean(distances) * 10\n","kernel.sigma.assign(sigma)\n","print(f\"Initial sigma: {kernel.sigma.numpy()}\")\n","\n","# Train the model\n","kqm_class.fit(X_train, y_train, validation_data=(X_val[:1000], y_val[:1000]), epochs=10, batch_size=128, verbose=1)\n","print(f\"Final sigma: {kernel.sigma.numpy()}\")\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":24,"source":["# Visualize the points in the feature space\n","plt.scatter(encoder(X_train)[:, 0], encoder(X_train)[:, 1], alpha=0.5, c=y_train, cmap=plt.cm.coolwarm)\n","\n","# plot the prototypes\n","plt.scatter(kqm_unit.c_x.numpy()[:, 0], kqm_unit.c_x.numpy()[:, 1], c='k', marker='X', s=50)\n","plt.show()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":25,"source":["# Evaluate the classifier\n","score = kqm_class.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","score = kqm_class.evaluate(X_train, y_train, verbose=0)\n","print('Train loss:', score[0])\n","print('Train accuracy:', score[1])\n","print(f'Sigma: {kernel.sigma.numpy()}')\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["Now we will train a density estimation model using the KQMDenEstModel2 class and keeping the encoder fix. This model maximizes the joint likelihood of inputs and outputs. The goal is to obtain more representative prototypes, since the prototypes learned using the classification model are not representative of the data distribution, since them are optimized to maximize the classification accuracy.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":26,"source":["# join X and y using a one-hot encoding for y\n","n_comp = 256\n","Xy_train = np.concatenate((encoder(X_train), np.eye(10)[y_train]), axis=1)\n","\n","kqmd_model2 = kqm.KQMDenEstModel2(2, 10, kernel.sigma.numpy(), n_comp=n_comp)\n","optimizer = tf.keras.optimizers.Adam(lr=5e-4)\n","encoder.trainable = False\n","kqmd_model2.compile(optimizer=optimizer)\n","kqmd_model2.predict(Xy_train[0:1]) # initialize the model\n","\n","# Assign the prototypes\n","idx = np.random.randint(Xy_train.shape[0], size=n_comp)\n","kqmd_model2.kqmover.c_x.assign(Xy_train[idx])\n","\n","print(f'Initial sigma: {kqmd_model2.kernel_x.sigma.numpy()}')\n","\n","#kqmd_model2.kqmover.c_x.assign(tf.concat([kqm_unit.c_x, kqm_unit.c_y], axis=1))\n","kqmd_model2.fit(Xy_train, epochs=20, verbose=0, batch_size=128)\n","\n","print(f'Final sigma: {kqmd_model2.kernel_x.sigma.numpy()}')\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":27,"source":["# Visualize the points in the feature space \n","plt.scatter(encoder(X_train)[:, 0], encoder(X_train)[:, 1], alpha=0.5, c=y_train, cmap=plt.cm.coolwarm)\n","# plot the prototypes\n","#comp_w = tf.clip_by_value(kqmd_model2.kqmover.comp_w, 1e-10, 1).numpy()\n","comp_w = tf.abs(kqmd_model2.kqmover.comp_w).numpy()\n","comp_w = comp_w \/ tf.reduce_sum(comp_w)\n","prototypes = kqmd_model2.kqmover.c_x.numpy()[comp_w > 0.001]\n","plt.scatter(prototypes[:, 0], prototypes[:, 1], c='k', marker='X', s=50)\n","plt.show()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":28,"source":["import seaborn as sns\n","g = sns.kdeplot(x=kqmd_model2.kqmover.c_x.numpy()[:, 0], y=kqmd_model2.kqmover.c_x.numpy()[:, 1], \n","                hue=np.argmax(kqmd_model2.kqmover.c_x[:, 2:].numpy(), axis=1),\n","                  fill=False, legend=True, bw_adjust=1.7, palette='muted')\n","g.set_xlabel(\"$x'_0$\", fontsize=14)\n","g.set_ylabel(\"$x'_1$\", fontsize=14)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":29,"source":["kqmd_model2.kqmover.c_x.shape\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["Now we evaluate the classification performance of the learned prototypes. We can see that the classification accuracy slightly improved with respect to the classification model.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":30,"source":["\n","kernel_2 = kqm.RBFKernelLayer(sigma=kqmd_model2.kernel_x.sigma.numpy(), dim=2)\n","kqm_unit_2 = kqm.KQMUnit(kernel=kernel_2, dim_x=encoded_size, dim_y=10, n_comp=n_comp)\n","rho_x_2 = kqm.pure2dm(encoded)\n","rho_y_2 = kqm_unit_2(rho_x_2)\n","probs_2 = kqm.dm2discrete(rho_y_2)\n","kqm_class_2 = Model(inputs, probs_2)\n","kqm_class_2.compile(optimizer=optimizers.Adam(lr=5e-4),\n","                    loss=losses.sparse_categorical_crossentropy,\n","                    metrics=[metrics.sparse_categorical_accuracy])\n","kqm_unit_2.c_x.assign(kqmd_model2.kqmover.c_x[:, :2])\n","kqm_unit_2.c_y.assign(kqmd_model2.kqmover.c_x[:, 2:])\n","kqm_unit_2.comp_w.assign(kqmd_model2.kqmover.comp_w);\n","\n","# Evaluate the classifier\n","score = kqm_class_2.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","score = kqm_class.evaluate(X_train, y_train, verbose=0)\n","print('Train loss:', score[0])\n","print('Train accuracy:', score[1])\n","print(f'Sigma: {kernel.sigma.numpy()}')\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["## Generator model\n","\n","We can build a generator taking advantage of the symmetry of the KQMUnit. Basically we change the role of the x-prototypes and the y-prototypes. The entry to the model will one-hot-encoded vector that represents the class of the digit to generate. The output of the KQMUnit will be a probability distribution on the latent space. We can sample from this distribution to generate new samples that we will decode to obtain the generated digit. First we will train a decoder.\n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":31,"source":["def create_decoder(base_depth, encoded_size):\n","    decoder = tf.keras.Sequential([\n","        tfkl.InputLayer(input_shape=[encoded_size]),\n","        tfkl.Reshape([1, 1, encoded_size]),\n","        tfkl.Conv2DTranspose(2 * base_depth, 7, strides=1,\n","                            padding='valid', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2DTranspose(2 * base_depth, 5, strides=1,\n","                            padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2DTranspose(2 * base_depth, 5, strides=2,\n","                            padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n","                            padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2DTranspose(base_depth, 5, strides=2,\n","                            padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2DTranspose(base_depth, 5, strides=1,\n","                            padding='same', activation=tf.nn.leaky_relu),\n","        tfkl.Conv2D(filters=1, kernel_size=5, strides=1,\n","                    padding='same', activation=None),\n","    ])\n","    return decoder\n","\n","decoder = create_decoder(base_depth, encoded_size)\n","\n","ae_model = tf.keras.Sequential([inputs,\n","                            encoder,\n","                            decoder                            \n","                           ])\n","\n","ae_model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n","            loss=losses.BinaryCrossentropy(from_logits=True))\n","\n","ae_model.fit(X_train, X_train, epochs=10, batch_size=32, verbose=1)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"attachments":{},"source":["Let's plot some generated digits. The digits generated from the autoencoder are fuzzy because of the small dimension of the latent space. \n"],"cell_type":"markdown"},{"metadata":{"collapsed":false},"execution_count":32,"source":["display_imgs(X_test[20:30])\n","prediction = ae_model.predict(X_test[20:30])\n","\n","prediction = tf.nn.sigmoid(prediction)\n","display_imgs(prediction)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":33,"source":["# Create a generator classifier model using a KQMUnit\n","\n","inputs_gen = Input(shape=(10,))\n","kernel_gen = kqm.CosineKernelLayer()\n","kqm_unit_gen = kqm.KQMUnit(kernel=kernel_gen, dim_x=10, dim_y=2, n_comp=n_comp)\n","rho_y_gen = kqm.pure2dm(inputs_gen)\n","rho_x_gen = kqm_unit_gen(rho_y_gen)\n","kqm_gen = Model(inputs_gen, rho_x_gen)\n","kqm_unit_gen.c_x.assign(kqm_unit_2.c_y)\n","kqm_unit_gen.c_y.assign(kqm_unit_2.c_x)\n","kqm_unit_gen.comp_w.assign(kqm_unit_2.comp_w)\n","kqm_gen.compile()\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":34,"source":["# Generate a set of distributions from the model for each digit\n","distribs = kqm.dm2distrib(kqm_gen(tf.eye(10)), sigma=kernel.sigma)\n","# Sample from the distributions\n","samples = []\n","for i in range(10):\n","    sample = distribs[i].sample(10).numpy()\n","    samples.append(sample)\n","    # Show the decoded images\n","    decoded_imgs = decoder(sample)\n","    decoded_imgs = tf.nn.sigmoid(decoded_imgs)\n","    display_imgs(decoded_imgs)\n","\n","# plot the samples in the feature space\n","samples = np.concatenate(samples)\n","plt.scatter(encoder(X_train)[:, 0], encoder(X_train)[:, 1], alpha=0.5, c=y_train, cmap=plt.cm.coolwarm)\n","plt.scatter(samples[:, 0], samples[:, 1], c='k', marker='X', s=50)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":38,"source":["# Generate a set of distributions from the model for each digit\n","y_distrib = np.zeros((1,10))\n","y_distrib[0, 0] = 1 \/ np.sqrt(2)\n","y_distrib[0, 1] = 1 \/ np.sqrt(2)\n","x_distrib = kqm.dm2distrib(kqm_gen(y_distrib), sigma=kernel.sigma)[0]\n","# Sample from the distributions\n","sample = x_distrib.sample(100).numpy()\n","# Show the decoded images\n","decoded_imgs = decoder(sample[:10])\n","decoded_imgs = tf.nn.sigmoid(decoded_imgs)\n","display_imgs(decoded_imgs)\n","\n","# plot the samples in the feature space\n","plt.scatter(encoder(X_train)[:, 0], encoder(X_train)[:, 1], alpha=0.5, c=y_train, cmap=plt.cm.coolwarm)\n","plt.scatter(sample[:, 0], sample[:, 1], c='k', marker='X', s=50)\n"],"cell_type":"code"},{"metadata":{"collapsed":false},"execution_count":36,"source":["for i in range(2):\n","    sample = distribs[i].sample(1000).numpy()\n","    g = sns.kdeplot(x=sample[:, 0], y=sample[:, 1], \n","                    fill=False, legend=False, bw_adjust=1, palette='muted')\n","g.set_xlabel(\"$x'_0$\", fontsize=14)\n","g.set_ylabel(\"$x'_1$\", fontsize=14)\n","g.set_xlim(-0.4, 0.4)\n","g.set_ylim(-0.4, 0.4)\n"],"cell_type":"code"}]}